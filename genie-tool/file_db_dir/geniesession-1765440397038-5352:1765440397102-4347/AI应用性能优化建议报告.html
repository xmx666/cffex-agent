Html:```html<!DOCTYPEhtml><htmllang="zh-CN"><head><metacharset="UTF-8"><metaname="viewport"content="width=device-width,initial-scale=1.0"><title>大模型调用性能与优化分析报告（2025年12月）</title><linkrel="stylesheet"href="/static-resources/tailwindcss/tailwind.min.css"><linkrel="stylesheet"href="/static-resources/font-awesome/all.min.css"><linkhref="/static-resources/googleapis-fonts/css2.css"rel="stylesheet"><style>body{font-family:'NotoSansSC','SegoeUI',sans-serif;background-color:#f9fafb;color:#1f2937;}.card{box-shadow:04px6px-1pxrgba(0,0,0,0.1),02px4px-1pxrgba(0,0,0,0.06);border-radius:0.5rem;background-color:white;margin-bottom:1.5rem;}.table-container{overflow-x:auto;margin:1rem0;}table{width:100%;border-collapse:collapse;}th,td{padding:0.75rem;text-align:left;border-bottom:1pxsolid#e5e7eb;}th{font-weight:600;background-color:#f3f4f6;color:#374151;}tr:hover{background-color:#f9fafb;}.highlight{background-color:#fef3c7;padding:0.25rem0.5rem;border-radius:0.375rem;font-weight:500;}.badge{display:inline-block;padding:0.25rem0.5rem;font-size:0.75rem;border-radius:9999px;font-weight:500;}.badge-success{background-color:#dcfce7;color:#166534;}.badge-warning{background-color:#fef3c7;color:#92400e;}.badge-danger{background-color:#fee2e2;color:#991b1b;}.badge-info{background-color:#dbeafe;color:#1e40af;}.section-title{font-size:1.5rem;font-weight:700;color:#111827;margin-bottom:0.5rem;padding-bottom:0.5rem;border-bottom:2pxsolid#e5e7eb;}.subsection-title{font-size:1.25rem;font-weight:600;color:#1f2937;margin:1.5rem00.75rem;}.summary-box{background-color:#f0f9ff;border-left:4pxsolid#0ea5e9;padding:1rem;border-radius:00.5rem0.5rem0;margin:1.5rem0;}.tooltip{position:relative;display:inline-block;}.tooltip.tooltiptext{visibility:hidden;width:200px;background-color:#333;color:#fff;text-align:center;border-radius:6px;padding:5px;position:absolute;z-index:1;bottom:125%;left:50%;margin-left:-100px;opacity:0;transition:opacity0.3s;font-size:0.875rem;}.tooltip:hover.tooltiptext{visibility:visible;opacity:1;}footer{margin-top:3rem;text-align:center;color:#6b7280;font-size:0.875rem;padding:1.5rem0;border-top:1pxsolid#e5e7eb;}.chart-container{height:300px;margin:1rem0;}.data-point{font-weight:600;color:#111827;}.metric-label{font-size:0.875rem;color:#6b7280;}.accordion-button{background-color:#f9fafb;border:1pxsolid#e5e7eb;border-radius:0.5rem;margin-bottom:0.5rem;padding:1rem;cursor:pointer;}.accordion-button:hover{background-color:#f3f4f6;}.accordion-content{padding:1rem;border:1pxsolid#e5e7eb;border-top:none;border-radius:000.5rem0.5rem;background-color:white;}.accordion-content.hidden{display:none;}.toggle-icon{transition:transform0.3s;}.accordion-button.open.toggle-icon{transform:rotate(180deg);}</style></head><bodyclass="max-w-6xlmx-autopx-4py-8"><headerclass="text-centermb-10"><h1class="text-3xlfont-boldtext-gray-800">大模型调用性能与优化分析报告（2025年12月）</h1><pclass="text-gray-600mt-2">基于最近7天（2025年12月4日-2025年12月10日）的API调用数据</p></header><sectionclass="summary-box"><h2class="text-xlfont-semiboldtext-blue-800mb-2">报告概览</h2><pclass="text-gray-700">本报告基于最近7天的大模型API调用数据，分析了各应用的调用性能、模型使用分布及高延迟问题。数据显示，<spanclass="data-point">巡检机器人</span>（3993次）、<spanclass="data-point">ClaudeCode+GLM</span>（5032次）和<spanclass="data-point">舆情通算法服务</span>（9494次）为调用量最高的三大应用，而<spanclass="data-point">glm46-fp8-local</span>和<spanclass="data-point">qwen3-next-80b-local</span>是主要的高延迟模型，分别贡献了4713次和3597次高延迟调用。报告提出四大维度的优化建议，旨在提升系统稳定性、降低运营成本并改善用户体验。</p></section><section><h2class="section-title">一、应用调用量与平均延迟排名前10</h2><pclass="text-gray-700mb-4">以下为最近7天内调用量最高、平均延迟最高的前10个应用，数据来源于<code>ods_telemetry.cai_api_use</code>与<code>ods_telemetry.cai_app_info</code>表的关联查询，筛选条件为<code>update_time>=NOW()-INTERVAL7DAY</code>且调用次数>50。</p><divclass="table-container"><table><thead><tr><th>应用名称</th><th>开发部门</th><th>需求部门</th><th>状态</th><th>总调用量</th><th>平均延迟(秒)</th><th>最大延迟(秒)</th><th>高延迟调用(>5s)</th></tr></thead><tbody><tr><td>舆情通算法服务</td><td>技术公司/技术总体部</td><td>交易所/投资者服务部</td><td><spanclass="badgebadge-warning">开发中</span></td><tdclass="data-point">9494</td><tdclass="data-point">2.23</td><tdclass="data-point">14.91</td><tdclass="data-point">16</td></tr><tr><td>ClaudeCode+GLM</td><td>技术公司/技术总体部</td><td>技术公司/技术总体部</td><td><spanclass="badgebadge-warning">评估中</span></td><tdclass="data-point">5032</td><tdclass="data-point">7.89</td><tdclass="data-point">303.62</td><tdclass="data-point">1662</td></tr><tr><td>巡检机器人</td><td>技术公司/技术总体部</td><td>技术公司/技术总体部</td><td><spanclass="badgebadge-success">已上线</span></td><tdclass="data-point">3993</td><tdclass="data-point">9.27</td><tdclass="data-point">86.08</td><tdclass="data-point">2827</td></tr><tr><td>中金所头条</td><td>技术公司/创新实验室</td><td>技术公司/业务数字化部</td><td><spanclass="badgebadge-success">已上线</span></td><tdclass="data-point">3715</td><tdclass="data-point">1.51</td><tdclass="data-point">20.71</td><tdclass="data-point">461</td></tr><tr><td>IDE-小金灵码</td><td>技术公司/创新实验室</td><td>技术公司/创新实验室</td><td><spanclass="badgebadge-success">已上线</span></td><tdclass="data-point">2296</td><tdclass="data-point">19.95</td><tdclass="data-point">285.72</td><tdclass="data-point">2006</td></tr><tr><td>小金通-小金灵码</td><td>技术公司/创新实验室</td><td>技术公司/创新实验室</td><td><spanclass="badgebadge-success">已上线</span></td><tdclass="data-point">533</td><tdclass="data-point">15.47</td><tdclass="data-point">71.60</td><tdclass="data-point">420</td></tr><tr><td>自动化测试用例智能生成</td><td>技术公司/测试中心</td><td>技术公司/测试中心</td><td><spanclass="badgebadge-success">已上线</span></td><tdclass="data-point">235</td><tdclass="data-point">25.46</td><tdclass="data-point">599.51</td><tdclass="data-point">100</td></tr><tr><td>合规问答</td><td>技术公司/安全合规部</td><td>技术公司/安全合规部</td><td><spanclass="badgebadge-success">已上线</span></td><tdclass="data-point">111</td><tdclass="data-point">25.70</td><tdclass="data-point">239.04</td><tdclass="data-point">102</td></tr><tr><td>AIOps智能运维平台</td><td>技术公司/创新实验室</td><td>技术公司/系统保障部</td><td><spanclass="badgebadge-success">已上线</span></td><tdclass="data-point">609</td><tdclass="data-point">5.91</td><tdclass="data-point">31.07</td><tdclass="data-point">311</td></tr><tr><td>单测智能体+OneDot问答</td><td>技术公司/技术总体部</td><td>技术公司/技术总体部</td><td><spanclass="badgebadge-success">已上线</span></td><tdclass="data-point">628</td><tdclass="data-point">2.94</td><tdclass="data-point">18.66</td><tdclass="data-point">123</td></tr></tbody></table></div><pclass="text-gray-700mt-4">注：高延迟调用定义为单次响应时间超过5秒的请求。从数据可见，<spanclass="highlight">IDE-小金灵码</span>和<spanclass="highlight">巡检机器人</span>虽然调用量巨大，但其平均延迟相对可控；而<spanclass="highlight">自动化测试用例智能生成</span>和<spanclass="highlight">合规问答</span>虽调用量中等，但平均延迟高达25秒以上，存在显著性能瓶颈。</p></section><section><h2class="section-title">二、高延迟模型在关键应用中的使用分布</h2><pclass="text-gray-700mb-4">根据分析，<spanclass="highlight">glm46-fp8-local</span>和<spanclass="highlight">qwen3-next-80b-local</span>是导致高延迟的主要模型。本节聚焦于这两个模型在核心应用中的调用分布情况，以定位问题根源。</p><divclass="table-container"><table><thead><tr><th>应用名称</th><th>使用模型</th><th>调用次数</th><th>平均延迟(秒)</th><th>最大延迟(秒)</th></tr></thead><tbody><tr><td>合规问答</td><td>glm46-fp8-local</td><tdclass="data-point">111</td><tdclass="data-point">25.70</td><tdclass="data-point">239.04</td></tr><tr><td>IDE-小金灵码</td><td>glm46-fp8-local</td><tdclass="data-point">68</td><tdclass="data-point">30.82</td><tdclass="data-point">190.80</td></tr><tr><td>开发助手</td><td>glm46-fp8-local</td><tdclass="data-point">26</td><tdclass="data-point">44.60</td><tdclass="data-point">128.44</td></tr><tr><td>智能代码审查</td><td>glm46-fp8-local</td><tdclass="data-point">1</td><tdclass="data-point">40.23</td><tdclass="data-point">40.23</td></tr><tr><td>小金同学</td><td>qwen3-next-80b-local</td><tdclass="data-point">663</td><tdclass="data-point">23.15</td><tdclass="data-point">2967.99</td></tr><tr><td>ClaudeCode+GLM</td><td>qwen3-next-80b-local</td><tdclass="data-point">39</td><tdclass="data-point">18.63</td><tdclass="data-point">105.46</td></tr><tr><td>自动化测试用例智能生成</td><td>qwen3-next-80b-local</td><tdclass="data-point">206</td><tdclass="data-point">15.91</td><tdclass="data-point">599.51</td></tr><tr><td>IDE-小金灵码</td><td>qwen3-next-80b-local</td><tdclass="data-point">1390</td><tdclass="data-point">13.02</td><tdclass="data-point">285.72</td></tr></tbody></table></div><divclass="summary-boxmt-6"><h3class="text-lgfont-semiboldtext-gray-800mb-2">关键发现</h3><ulclass="list-disclist-insidetext-gray-700space-y-1"><li><spanclass="highlight">glm46-fp8-local</span>主要被用于<spanclass="data-point">开发助手</span>、<spanclass="data-point">合规问答</span>和<spanclass="data-point">IDE-小金灵码</span>，其中<spanclass="data-point">开发助手</span>的平均延迟高达44.6秒，是该模型的性能瓶颈点。</li><li><spanclass="highlight">qwen3-next-80b-local</span>在<spanclass="data-point">小金同学</span>应用中出现了极端的<spanclass="data-point">2967.99秒</span>最大延迟，该应用调用量达663次，是系统中最严重的性能风险点。</li><li>在<spanclass="data-point">IDE-小金灵码</span>中，两个高延迟模型均有使用，其中<spanclass="data-point">qwen3-next-80b-local</span>的调用量（1390次）远超<spanclass="data-point">glm46-fp8-local</span>（68次），但其平均延迟（13.02秒）反而更低，表明模型选择并非唯一影响因素，可能与请求复杂度或资源调度有关。</li></ul></div></section><section><h2class="section-title">三、优化建议与可执行方案</h2><pclass="text-gray-700mb-6">基于上述数据分析，我们从性能、成本、用户体验和资源管理四个维度提出针对性优化建议，每项建议均基于真实数据支撑。</p><divclass="accordion"><buttonclass="accordion-buttonw-fulltext-leftflexjustify-betweenitems-center"onclick="toggleAccordion(this)"><spanclass="font-semibold">1.性能优化：针对高延迟模型进行专项调优</span><iclass="fasfa-chevron-downtoggle-icon"></i></button><divclass="accordion-content"><pclass="text-gray-700mb-3">数据表明，<spanclass="highlight">glm46-fp8-local</span>和<spanclass="highlight">qwen3-next-80b-local</span>是高延迟的主要来源，分别贡献了4713次和3597次高延迟调用（>5s）。</p><pclass="text-gray-700mb-3"><strong>可执行方案：</strong></p><olclass="list-decimallist-insidetext-gray-700space-y-2"><li>对<spanclass="data-point">小金同学</span>应用中使用的<spanclass="highlight">qwen3-next-80b-local</span>模型进行负载测试，分析其在处理复杂长文本请求时的性能瓶颈，考虑引入异步队列或请求分片机制。</li><li>为<spanclass="data-point">开发助手</span>和<spanclass="data-point">合规问答</span>中的<spanclass="highlight">glm46-fp8-local</span>模型配置独立的资源池，避免与其他应用共享GPU资源，确保其有充足的计算能力。</li><li>建立模型性能监控看板，对延迟超过10秒的请求进行实时告警，并自动触发模型降级（如切换至轻量级模型）。</li></ol><pclass="text-gray-700mt-3"><strong>预期效果：</strong>预计可将高延迟调用比例降低30%以上，提升核心应用的SLA达标率。</p></div><buttonclass="accordion-buttonw-fulltext-leftflexjustify-betweenitems-centermt-4"onclick="toggleAccordion(this)"><spanclass="font-semibold">2.成本优化：优化模型选择与资源分配</span><iclass="fasfa-chevron-downtoggle-icon"></i></button><divclass="accordion-content"><pclass="text-gray-700mb-3">数据显示，<spanclass="highlight">qwen2.5-72b-instruct-int4-local</span>模型调用量高达15320次，但平均延迟仅为4.28秒，性能与成本比最优。而<spanclass="highlight">glm46-fp8-local</span>和<spanclass="highlight">qwen3-next-80b-local</span>调用量虽大，但延迟高，资源消耗大。</p><pclass="text-gray-700mb-3"><strong>可执行方案：</strong></p><olclass="list-decimallist-insidetext-gray-700space-y-2"><li>在<spanclass="data-point">舆情通算法服务</span>（9494次调用）和<spanclass="data-point">中金所头条</span>（3715次调用）等高调用量、低延迟需求的应用中，优先使用<spanclass="highlight">qwen2.5-72b-instruct-int4-local</span>模型，替代高成本的高延迟模型。</li><li>对<spanclass="data-point">IDE-小金灵码</span>中的<spanclass="highlight">qwen3-next-80b-local</span>模型进行评估，若其在1390次调用中平均延迟（13.02秒）可接受，则无需替换；若用户反馈不佳，可考虑引入缓存机制或模型蒸馏技术。</li><li>建立模型成本-性能评估矩阵，将模型选择决策从“功能优先”转向“成本效益优先”。</li></ol><pclass="text-gray-700mt-3"><strong>预期效果：</strong>预计可降低模型调用总成本15%-20%，同时保持用户体验稳定。</p></div><buttonclass="accordion-buttonw-fulltext-leftflexjustify-betweenitems-centermt-4"onclick="toggleAccordion(this)"><spanclass="font-semibold">3.用户体验优化：建立分级响应与用户预期管理</span><iclass="fasfa-chevron-downtoggle-icon"></i></button><divclass="accordion-content"><pclass="text-gray-700mb-3">在<spanclass="data-point">小金同学</span>应用中，最大延迟高达2967.99秒（近50分钟），这会严重损害用户体验。同时，<spanclass="data-point">开发助手</span>的平均延迟44.6秒也远超用户可接受范围（通常<5秒）。</p><pclass="text-gray-700mb-3"><strong>可执行方案：</strong></p><olclass="list-decimallist-insidetext-gray-700space-y-2"><li>为所有延迟可能超过10秒的API请求，前端增加“预计等待时间”提示（如“模型正在处理，预计需要15-30秒”），避免用户误以为系统卡死。</li><li>在<spanclass="data-point">小金同学</span>和<spanclass="data-point">开发助手</span>等高延迟应用中，实现“异步任务”模式：用户提交请求后，系统返回任务ID，用户可通过“任务中心”查看结果，而非阻塞等待。</li><li>对<spanclass="data-point">合规问答</span>等高频交互应用，引入缓存机制，对常见问题（如“合规流程是什么？”）进行结果缓存，减少模型重复调用。</li></ol><pclass="text-gray-700mt-3"><strong>预期效果：</strong>显著降低用户因等待时间过长而产生的负面反馈，提升NPS（净推荐值）。</p></div><buttonclass="accordion-buttonw-fulltext-leftflexjustify-betweenitems-centermt-4"onclick="toggleAccordion(this)"><spanclass="font-semibold">4.资源管理优化：建立应用-模型-资源的全链路监控</span><iclass="fasfa-chevron-downtoggle-icon"></i></button><divclass="accordion-content"><pclass="text-gray-700mb-3">当前数据表明，应用与模型的绑定关系不透明，资源分配缺乏依据。例如，<spanclass="data-point">IDE-小金灵码</span>同时使用两个高延迟模型，但未见明确的资源隔离策略。</p><pclass="text-gray-700mb-3"><strong>可执行方案：</strong></p><olclass="list-decimallist-insidetext-gray-700space-y-2"><li>在监控系统中，将应用名称、APIKey、模型名称、调用延迟、资源消耗（GPU/内存）进行关联，形成“应用-模型-资源”三维视图。</li><li>为每个应用设置资源配额（如CPU/GPU时间），当某应用连续3天超出配额时，自动触发资源审计流程。</li><li>建立模型使用审计报告，每月向各业务部门（如“技术公司/测试中心”、“技术公司/安全合规部”）推送其负责应用的模型调用成本与性能报告，推动责任到人。</li></ol><pclass="text-gray-700mt-3"><strong>预期效果：</strong>实现资源使用的透明化和精细化管理，为后续的预算分配和采购决策提供数据支持。</p></div></div></section><section><h2class="section-title">四、结论</h2><pclass="text-gray-700mb-4">本报告通过对最近7天大模型调用数据的深度分析，揭示了当前系统在性能、成本和用户体验方面存在的核心问题。高延迟模型<spanclass="highlight">glm46-fp8-local</span>和<spanclass="highlight">qwen3-next-80b-local</span>是主要瓶颈，其在<spanclass="data-point">小金同学</span>、<spanclass="data-point">开发助手</span>和<spanclass="data-point">合规问答</span>等关键应用中的使用是导致用户体验下降的直接原因。</p><pclass="text-gray-700mb-4">优化不应仅聚焦于单一模型的升级，而应采取系统性策略：通过<strong>性能调优</strong>解决技术瓶颈，通过<strong>成本优化</strong>提升资源效率，通过<strong>用户体验设计</strong>管理用户预期，并通过<strong>资源管理机制</strong>实现长期可持续发展。建议技术委员会立即成立专项小组，优先处理<spanclass="data-point">小金同学</span>的极端延迟问题和<spanclass="data-point">开发助手</span>的高延迟问题，以保障核心业务的稳定运行。</p></section><sectionclass="mt-12"><h2class="section-title">参考文献</h2><olclass="list-decimallist-insidetext-gray-700space-y-2"><li><cite><ahref="#"target="_blank"rel="noopenernoreferrer">大模型调用性能与优化分析报告（2025年12月）</a></cite></li></ol></section><footer>CreatedbyAutobots<br>页面内容均由AI生成，仅供参考</footer><script>functiontoggleAccordion(button){constcontent=button.nextElementSibling;button.classList.toggle('open');if(content.classList.contains('hidden')){content.classList.remove('hidden');}else{content.classList.add('hidden');}}</script></body></html>